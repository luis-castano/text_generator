{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/text/tutorials/text_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file(\n",
    "    'shakespeare.txt',\n",
    "    'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab),\n",
    "    mask_token=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(),\n",
    "    invert=True,\n",
    "    mask_token=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1], dtype=int64)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "    print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "    print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list('Tensorflow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(f'Input: {text_from_ids(input_example).numpy()}')\n",
    "    print(f'Target: {text_from_ids(target_example).numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset (TF data is designed to work with possibly infinite sequences, so it doesn't attempt to shuffle the entire sequence in memory. Instead, it maintains a buffer in which it shuffles elements)\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            rnn_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True\n",
    "        )\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, '# (batch_size, sequence_length, vocab_size)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  16896     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,022,850\n",
      "Trainable params: 4,022,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25, 29, 52, 20, 34, 22, 10, 39, 60, 26, 22, 61,  1, 29, 56,  9,  6,\n",
       "       37, 47, 43, 53, 60, 65,  8, 64, 47, 63, 62, 24,  2, 23, 24, 62, 55,\n",
       "       46, 11, 22, 44, 36, 15, 42,  0, 48, 60, 27, 49, 22, 24, 46, 39, 42,\n",
       "       26, 30, 15, 62, 60, 37, 49,  9, 64, 44, 11, 11, 49, 31, 59,  3, 44,\n",
       "       36, 52, 44, 40, 10,  2, 53, 20, 12, 59, 26,  7,  5, 20, 58, 63, 46,\n",
       "       40, 29,  8, 50, 48, 14, 32,  0, 63, 10, 14, 56, 52, 42, 57],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "b\"OP:\\nPeace have they made with him indeed, my lord.\\n\\nKING RICHARD II:\\nO villains, vipers, damn'd with\"\n",
      "Next char predictions:\n",
      "b\"LPmGUI3ZuMIv\\nPq.'Xhdnuz-yhxwK JKwpg:IeWBc[UNK]iuNjIKgZcMQBwuXj.ye::jRt!eWmea3 nG;tM,&GsxgaP-kiAS[UNK]x3Aqmcr\"\n"
     ]
    }
   ],
   "source": [
    "print(f'Input:\\n{text_from_ids(input_example_batch[0]).numpy()}')\n",
    "print(f'Next char predictions:\\n{text_from_ids(sampled_indices).numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'ckpts/training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt{epoch}')\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "172/172 [==============================] - 9s 40ms/step - loss: 2.7031\n",
      "Epoch 2/20\n",
      "172/172 [==============================] - 8s 40ms/step - loss: 1.9848\n",
      "Epoch 3/20\n",
      "172/172 [==============================] - 10s 54ms/step - loss: 1.7073\n",
      "Epoch 4/20\n",
      "172/172 [==============================] - 10s 54ms/step - loss: 1.5480\n",
      "Epoch 5/20\n",
      "172/172 [==============================] - 10s 54ms/step - loss: 1.4496\n",
      "Epoch 6/20\n",
      "172/172 [==============================] - 9s 52ms/step - loss: 1.3830\n",
      "Epoch 7/20\n",
      "172/172 [==============================] - 10s 52ms/step - loss: 1.3297\n",
      "Epoch 8/20\n",
      "172/172 [==============================] - 10s 52ms/step - loss: 1.2853\n",
      "Epoch 9/20\n",
      "172/172 [==============================] - 10s 54ms/step - loss: 1.2452\n",
      "Epoch 10/20\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 1.2061\n",
      "Epoch 11/20\n",
      "172/172 [==============================] - 9s 49ms/step - loss: 1.1664\n",
      "Epoch 12/20\n",
      "172/172 [==============================] - 10s 53ms/step - loss: 1.1262\n",
      "Epoch 13/20\n",
      "172/172 [==============================] - 10s 56ms/step - loss: 1.0826\n",
      "Epoch 14/20\n",
      "172/172 [==============================] - 10s 55ms/step - loss: 1.0380\n",
      "Epoch 15/20\n",
      "172/172 [==============================] - 10s 56ms/step - loss: 0.9888\n",
      "Epoch 16/20\n",
      "172/172 [==============================] - 10s 55ms/step - loss: 0.9383\n",
      "Epoch 17/20\n",
      "172/172 [==============================] - 10s 56ms/step - loss: 0.8873\n",
      "Epoch 18/20\n",
      "172/172 [==============================] - 10s 55ms/step - loss: 0.8369\n",
      "Epoch 19/20\n",
      "172/172 [==============================] - 10s 55ms/step - loss: 0.7839\n",
      "Epoch 20/20\n",
      "172/172 [==============================] - 10s 53ms/step - loss: 0.7366\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "        # Create a mask to prevent '[UNK]' from being generated\n",
    "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            # Put a -inf at each bad index.\n",
    "            values=[-float('inf')] * len(skip_ids),\n",
    "            indices=skip_ids,\n",
    "\n",
    "            # Match the shape to the vocabulary\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "        # Convert strings to token IDs\n",
    "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "        # Run the model. predicted_logits.shape is [batch, char, next_char_logits]\n",
    "        predicted_logits, states = self.model(\n",
    "            inputs=input_ids,\n",
    "            states=states,\n",
    "            return_state=True\n",
    "        )\n",
    "\n",
    "        # Only use the last prediction\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits / self.temperature\n",
    "\n",
    "        # Apply the prediction mask: prevent '[UNK]'from being generated\n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "        # Sample the output logits to generate token IDs\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "        # Convert from token ids to characters\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "        # Return the characters and model state\n",
    "        return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Good mistress, and perish can make them not,\n",
      "But bid him should my use for vantage must rich:\n",
      "Then, God defend that I be yet but great oppression\n",
      "faith, she is an expire to take an impoes,\n",
      "Small me a liebunation of thy stoumphatice,\n",
      "That mocks about him.\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "His absence,\n",
      "He might have done thee how to cut off all the language.\n",
      "\n",
      "OXF ROWE:\n",
      "Undwere you be a purpose divited counsel.\n",
      "Reednesser, you have done waste it; what I profess\n",
      "theirs, offer me night. Hark you, spot that the vastles vast thereof.\n",
      "Sound trumpets! strike my fall, stark and safe, and perush thou\n",
      "The verge my boar's son, those though us thus\n",
      "falling in his stocking: his horse drave deliver?\n",
      "\n",
      "WARWICK:\n",
      "I go; hopping you must see your brother's shadow,\n",
      "Kings yet our battles with the sweets o' the mother,\n",
      "Cry humords that Richmond should be.\n",
      "Diret aly his follies is wither'd blood!\n",
      "\n",
      "LORD WILLOUGHBY:\n",
      "Base!' quoth he. Hence-ball'd and me!\n",
      "Just Warwick's daughter, since I till compare\n",
      "Her father's skining so here w \n",
      "________________________________________________________________________________\n",
      "Run time: 4.164364337921143\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "\n",
    "print(result[0].numpy().decode('utf-8'), '\\n' + '_' * 80)\n",
    "print(f'Run time: {end - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b\"ROMEO:\\nThat little farewell.\\n\\nLORD WILLOUGHBY:\\nThe most sufficitive toem of sensible,\\nHath held your court-a-morrow'd truture,\\nThat shake the law, cry whether had an emperier.\\n\\nDUKE OF YORK:\\nWhere is thy poor crowns!\\nDays nightly, cousin Destruetion!\\nNow Romeo, will he mury this fair cozen?\\n\\nBAPTISTA:\\nI am busied the law upon this bloody moon.\\n\\nTRANIO:\\nDidst welcome, Bush, and damn'd up so dreadful man?\\n\\nPost:\\nA jealous arrs at voices.\\nO, no, no, every grey, or else\\nI know not on this castle's torture,\\nThan dwell is father. Luringly to the sun,\\nSo far as life, for nothing but deadly sendet,\\nRone of Such God, he been an old traves; and to be\\ncolour'd fear, I do, and that have man;\\nThese high powerful profession shall pardon me,\\nNever take him here, how has your viratural companion?\\n\\nISABELLA:\\nJut never\\nYou that so broken fattle, whose death\\nTake in abtance, with all speed to die:\\nThere stands that seeking him to put a little.\\nOne of thyself swear guers star?\\nHowlord of ELEY:\\nBrought off commo\"\n",
      " b\"ROMEO:\\nThou whorest not that I am God as my enflain-chance\\nInto hour'd that sought to be avoided\\nTo answer 'I abstaced afore a\\npleasure home with the embracements of might hath gone both the stague.\\n\\nHENRY BOLINGBROKE:\\nGive me the sun: did I put on sequer ta'en\\nTo pluck all would show a safe disdains.\\n\\nANGELO:\\nI will avouch'd forth plays for thee--\\n\\nNurse:\\nBage and gone and present well the threlling gross.\\n\\nTailor:\\n\\nGRUMIO:\\nWhy would her dear denies? My banish'd HaStings you rained at\\nThe brother's son must I starve,\\nYour suffering up what we may be a sisterhood!\\n\\nDUKE VINCENTIO:\\nStandea, 'twas Sir, I have confed to dine myself.\\n\\nLUCIO:\\nShall I not have thee debase your heavens are yearts: be patient\\nHearty arrived at the gamposign. As with his\\nceparely, get us, good old Auteouble ravour.\\nWhere thou ask huncelful\\nEdward, sweet man, more than a king,\\nAnd bid us bottle, sweet add angly books,\\nHis rather return to the qualit when he wakes;\\nOur and the salphey's good direching, man!\\n\\nClown:\\nSof\"\n",
      " b\"ROMEO:\\nIs not so great, good news, my lord,\\nSweet full when as the will of your blessed sense.\\nWill you anon! for thy fall son would thou return'd,\\nDo high do stil and great to the supper-of; no soul\\nThat seeing how with the shops into the thread.\\nAnd lock'd wavering that the swords shall know be stit\\nOf this blood of the famour tale was suffer'd.\\n\\nLUCIO:\\nLed but gates from all the fire right wrong-hermed hammonds,\\nAs cracled him and hark! Thou art not hot;\\nThe ordering creature crowns it plebeipless\\nmanner. Come, come that his purple to the queen,\\nTo stay a vassel, I will rend and turn.\\n\\nBUCKINGHAM:\\nGod save your grace for so good a vice:\\nI'll have this cruel death be to eye-holden\\nTread. Thus my father had prove's 'go.\\n\\nISABELLA:\\nO, each Good comfort!\\n\\nAUFIDIUS:\\nAy, sir, so heavy so: I would not bulder\\nThough 'twas, on his sine should be attendance?\\n\\nBePHENSO:\\nFor never.\\n\\nESCALUS:\\nDoth not, mayor: these words is it to set\\nCorn to the Richmodd. Thou injured,' I\\nhave offended, all this is al\"\n",
      " b\"ROMEO:\\nIs love as mine honesty,\\nMy brother's loss, how they are patient, given!\\n\\nLADY GREY:\\nHere comes a is your brother's beam: son found a hope,\\nAnd for that noble wounds access of thee\\nTo do them Take and the Lord Julier,\\nWave-horse Bulknig Haplas, help to do some-hall of me.\\n\\nGREMIO:\\nO my boy, than only seal him up with her. Come on,\\nHow I would say, with that tends at home, by this good lord.\\n\\nKITARDIUS:\\nSweet sir! he!\\nGodging this business mother, I am talk'd and brook you.\\nCan you succeed that I have forgot--\\n\\nGLOUCESTER:\\nI know me from that art woo's words.\\n\\nDUCHESS OF YORK:\\nWhy, no dear throne, in these gone is:\\nWhen you are old Signorious queen,\\nDid I love Edward marry With forfer liberty,\\nRically out for voice, storm to him;\\nAnd by the nepless he marry here inquiry castally;\\nIf you sad help to prison' blood of us\\nto see my sovereign, my heart penticulate\\nTo make them thank you: You may surmain what I am,\\nrewires deliver us to be gracious crew, some time and grip on\\nThat hardly cha\"\n",
      " b\"ROMEO:\\nThou darisbert is no balm hath deal in fire revenge;\\nAnd Roman proofound as there deliver\\nTo have a husband there for me.\\nTo feed him you shall fetch the trunk for this!\\nGod bling of a warrant of the citizens,\\nYour whores and the most name, poor study, as the rest!\\n\\nPOMPEY:\\nPromise you true-at the lazentation.\\nWelcome shall well entreat of England.\\nAnd, lords, good monroy'd by Bannarding to my father.\\n\\nSICINIUS:\\nWhen, I do love my seat,\\nAnd knee beside where no manner here.\\n\\nGLOUCESTER:\\nCousin us. CAULIET:\\nO, know you what touches? would away twenvy stouch\\nSmile attocate to be yet discourse:\\nThis gold ere he blest thine ear;\\nThough sorrow shall be fear'd--water, though they can:\\nAnd, for one voices had one nail, one\\nWhat bereft was fight: that the sweet father, it\\nwas begoted all the sad study for thy friends\\nThat seeing such men fairly loss his maid:\\nSome offence we have then ales!\\nCursed them and welcome them for the queen;\\nFive pinous me arguments,\\nBut weep the offices of distants \"], shape=(5,), dtype=string) \n",
      "________________________________________________________________________________\n",
      "Run time: 4.102835178375244\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "\n",
    "print(result, '\\n' + '_' * 80)\n",
    "print(f'Run time: {end - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x000001849F8DB250>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/one_step\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/one_step\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, 'models/one_step')\n",
    "one_step_reloaded = tf.saved_model.load('models/one_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "If love be balm'd, proud, King Edward's drid,\n",
      "One nature, my master named,--there's sendence,\n",
      "Ere h\n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "    next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTraining(MyModel):\n",
    "    @tf.function\n",
    "    def train_step(self, inputs):\n",
    "        inputs, labels = inputs\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(inputs, training=True)\n",
    "            loss = self.loss(labels, predictions)\n",
    "            \n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "        return {'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomTraining(\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 10s 39ms/step - loss: 2.7252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x185dd94fd90>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Batch: 0 | Loss: 2.1749\n",
      "Epoch: 1 | Batch: 50 | Loss: 2.0676\n",
      "Epoch: 1 | Batch: 100 | Loss: 1.9495\n",
      "Epoch: 1 | Batch: 150 | Loss: 1.8234\n",
      "Epoch: 1 | Loss: 1.9935\n",
      "Time taken for 1 epoch: 8.39 sec\n",
      "________________________________________________________________________________\n",
      "Epoch: 2 | Batch: 0 | Loss: 1.8705\n",
      "Epoch: 2 | Batch: 50 | Loss: 1.7184\n",
      "Epoch: 2 | Batch: 100 | Loss: 1.6541\n",
      "Epoch: 2 | Batch: 150 | Loss: 1.6737\n",
      "Epoch: 2 | Loss: 1.7201\n",
      "Time taken for 1 epoch: 7.35 sec\n",
      "________________________________________________________________________________\n",
      "Epoch: 3 | Batch: 0 | Loss: 1.6355\n",
      "Epoch: 3 | Batch: 50 | Loss: 1.5523\n",
      "Epoch: 3 | Batch: 100 | Loss: 1.5101\n",
      "Epoch: 3 | Batch: 150 | Loss: 1.5382\n",
      "Epoch: 3 | Loss: 1.5593\n",
      "Time taken for 1 epoch: 8.67 sec\n",
      "________________________________________________________________________________\n",
      "Epoch: 4 | Batch: 0 | Loss: 1.4343\n",
      "Epoch: 4 | Batch: 50 | Loss: 1.4782\n",
      "Epoch: 4 | Batch: 100 | Loss: 1.4549\n",
      "Epoch: 4 | Batch: 150 | Loss: 1.4564\n",
      "Epoch: 4 | Loss: 1.4591\n",
      "Time taken for 1 epoch: 9.81 sec\n",
      "________________________________________________________________________________\n",
      "Epoch: 5 | Batch: 0 | Loss: 1.4198\n",
      "Epoch: 5 | Batch: 50 | Loss: 1.4227\n",
      "Epoch: 5 | Batch: 100 | Loss: 1.3797\n",
      "Epoch: 5 | Batch: 150 | Loss: 1.3860\n",
      "Epoch: 5 | Loss: 1.3906\n",
      "Time taken for 1 epoch: 9.86 sec\n",
      "________________________________________________________________________________\n",
      "Epoch: 6 | Batch: 0 | Loss: 1.3525\n",
      "Epoch: 6 | Batch: 50 | Loss: 1.3433\n",
      "Epoch: 6 | Batch: 100 | Loss: 1.3459\n",
      "Epoch: 6 | Batch: 150 | Loss: 1.3421\n",
      "Epoch: 6 | Loss: 1.3377\n",
      "Time taken for 1 epoch: 9.73 sec\n",
      "________________________________________________________________________________\n",
      "Epoch: 7 | Batch: 0 | Loss: 1.3025\n",
      "Epoch: 7 | Batch: 50 | Loss: 1.2949\n",
      "Epoch: 7 | Batch: 100 | Loss: 1.3309\n",
      "Epoch: 7 | Batch: 150 | Loss: 1.2997\n",
      "Epoch: 7 | Loss: 1.2929\n",
      "Time taken for 1 epoch: 9.82 sec\n",
      "________________________________________________________________________________\n",
      "Epoch: 8 | Batch: 0 | Loss: 1.2160\n",
      "Epoch: 8 | Batch: 50 | Loss: 1.2641\n",
      "Epoch: 8 | Batch: 100 | Loss: 1.2310\n",
      "Epoch: 8 | Batch: 150 | Loss: 1.2983\n",
      "Epoch: 8 | Loss: 1.2525\n",
      "Time taken for 1 epoch: 9.79 sec\n",
      "________________________________________________________________________________\n",
      "Epoch: 9 | Batch: 0 | Loss: 1.2010\n",
      "Epoch: 9 | Batch: 50 | Loss: 1.1942\n",
      "Epoch: 9 | Batch: 100 | Loss: 1.2263\n",
      "Epoch: 9 | Batch: 150 | Loss: 1.2189\n",
      "Epoch: 9 | Loss: 1.2129\n",
      "Time taken for 1 epoch: 9.96 sec\n",
      "________________________________________________________________________________\n",
      "Epoch: 10 | Batch: 0 | Loss: 1.1363\n",
      "Epoch: 10 | Batch: 50 | Loss: 1.1665\n",
      "Epoch: 10 | Batch: 100 | Loss: 1.1791\n",
      "Epoch: 10 | Batch: 150 | Loss: 1.1803\n",
      "Epoch: 10 | Loss: 1.1735\n",
      "Time taken for 1 epoch: 10.08 sec\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "mean = tf.metrics.Mean()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    mean.reset_states()\n",
    "\n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "        logs = model.train_step([inp, target])\n",
    "        mean.update_state(logs['loss'])\n",
    "\n",
    "        if batch_n % 50 == 0:\n",
    "            template = f'Epoch: {epoch + 1} | Batch: {batch_n} | Loss: {logs[\"loss\"]:.4f}'\n",
    "            print(template)\n",
    "\n",
    "    # Saving (checkpoint) the model every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "    print(f'Epoch: {epoch + 1} | Loss: {mean.result().numpy():.4f}')\n",
    "    print(f'Time taken for 1 epoch: {time.time() - start:.2f} sec')\n",
    "    print('_' * 80)\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('text_generator')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04f3bdd4d01082c832357f9acba26f462eab2a5ceff0a68f47bfe239ac069e3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
